{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "af9ffb68-d184-4c9d-9f31-e654040e87d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "2357193d-2cb6-482e-8105-eec5805d7ca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Simple neuron\n",
    "def neuron(input_value, weight, bias):\n",
    "    output = input_value * weight + bias\n",
    "    return output\n",
    "\n",
    "# Step 2: ReLU activation\n",
    "def relu(x):\n",
    "    return max(0, x)\n",
    "def leaky_relu(x):\n",
    "    return x if x>0 else 0.1*x \n",
    "def sigmoid(x):\n",
    "    denominator= 1+ np.exp(-x);\n",
    "    return 1/denominator\n",
    "def tanh(x):\n",
    "    numerator= np.exp(-x)-np.exp(x)\n",
    "    denominator= np.exp(-x)-np.exp(x);\n",
    "    return numerator/denominator\n",
    "def elu(x, alpha=1.0):\n",
    "    return x if x > 0 else alpha * (np.exp(x) - 1)\n",
    "def swish(x):\n",
    "    return x*sigmoid(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "e0093250-f8b3-4ca1-8ff6-5a39a0d88e42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neuron output before activation: 35.5\n",
      "Neuron output after ReLU: 35.5\n",
      "Neuron output after leaky_relu: 35.5\n",
      "Neuron output after sigmoid: 0.9999999999999996\n",
      "Neuron output after tanh: 1.0\n",
      "Neuron output after eLU: 35.5\n",
      "Neuron output after swish: 35.499999999999986\n"
     ]
    }
   ],
   "source": [
    "# Step 3: Try for multiple inputs\n",
    "inputs = [-2.0, -3.0, 1.5,5,2,1,3]\n",
    "weights = [0.5, -1.0, 2.0,5,1,-1,1]\n",
    "bias = 1.5\n",
    "\n",
    "# Calculate total sum\n",
    "total = sum([i * w for i, w in zip(inputs, weights)]) + bias\n",
    "\n",
    "# Apply activation\n",
    "relu_op = relu(total)\n",
    "leaky_relu_op = leaky_relu(total)\n",
    "sigmoid_op =sigmoid(total)\n",
    "tanh_op = tanh(total)\n",
    "elu_op = elu(total)\n",
    "swish_op =swish(total)\n",
    "\n",
    "print(\"Neuron output before activation:\", total)\n",
    "print(\"Neuron output after ReLU:\",relu_op)\n",
    "print(\"Neuron output after leaky_relu:\",leaky_relu_op)\n",
    "print(\"Neuron output after sigmoid:\",sigmoid_op)\n",
    "print(\"Neuron output after tanh:\",tanh_op)\n",
    "print(\"Neuron output after eLU:\",elu_op)\n",
    "print(\"Neuron output after swish:\",swish_op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "5092224f-dc30-4a24-90f6-3684fe72dc63",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid_derivative(x):\n",
    "    s = sigmoid(x)\n",
    "    return s * (1 - s)\n",
    "\n",
    "# Dataset\n",
    "X = np.array([[0,0], [0,1], [1,0], [1,1]])\n",
    "Y = np.array([[0], [1], [1], [0]])\n",
    "\n",
    "# Architecture\n",
    "input_size = 2\n",
    "hidden_size = 2\n",
    "output_size = 1\n",
    "\n",
    "# Weights and biases\n",
    "np.random.seed(42)\n",
    "W1 = np.random.randn(input_size, hidden_size)\n",
    "b1 = np.random.randn(hidden_size)\n",
    "W2 = np.random.randn(hidden_size, output_size)\n",
    "b2 = np.random.randn(output_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "88c312ae-c86e-4f6a-9419-75ec40ecbd7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 Loss: 0.2943508288500037\n",
      "\n",
      "Final output after training:\n",
      "[[0.44351298]\n",
      " [0.50973176]\n",
      " [0.51527102]\n",
      " [0.55288978]]\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "epochs = 1000\n",
    "lr = 0.1\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(epochs):\n",
    "    # Forward Pass\n",
    "    z1 = np.dot(X, W1) + b1\n",
    "    a1 = sigmoid(z1)\n",
    "    z2 = np.dot(a1, W2) + b2\n",
    "    a2 = sigmoid(z2)\n",
    "\n",
    "    # Compute Loss (MSE)\n",
    "    loss = np.mean((Y - a2)**2)\n",
    "\n",
    "    # dLoss/dW1 = dLoss/dA2 * dA2/dZ2 * dZ2/dA1 * dA1/dZ1 * dZ1/dW1   \n",
    "    #loss w.r.to w1 --> first loss blames activation 2 then it balmes --> w2 -> act1 -> w1\n",
    "\n",
    "    # Backward Pass\n",
    "    d_loss_a2 = -(Y - a2)       # Derivative of Loss w.r.t. a2 (prediction output).\n",
    "    d_a2_z2 = sigmoid_derivative(z2) # Derivative of activation at output layer.\n",
    "    d_z2_W2 = a1      #The derivative of z2 w.r.t. W2 is a1\n",
    "\n",
    "    d_loss_W2 = np.dot(d_z2_W2.T, d_loss_a2 * d_a2_z2)\n",
    "    d_loss_b2 = np.sum(d_loss_a2 * d_a2_z2, axis=0)\n",
    "\n",
    "    d_z2_a1 = W2\n",
    "    d_a1_z1 = sigmoid_derivative(z1)\n",
    "    \n",
    "    d_loss_W1 = np.dot(X.T, ((np.dot(d_loss_a2 * d_a2_z2, d_z2_a1.T)) * d_a1_z1))\n",
    "    d_loss_b1 = np.sum((np.dot(d_loss_a2 * d_a2_z2, d_z2_a1.T)) * d_a1_z1, axis=0)\n",
    "\n",
    "    # Update weights\n",
    "    W2 -= lr * d_loss_W2\n",
    "    b2 -= lr * d_loss_b2\n",
    "    W1 -= lr * d_loss_W1\n",
    "    b1 -= lr * d_loss_b1\n",
    "\n",
    "    # Print loss every 1000 epochs\n",
    "    if epoch % 100 == 0:\n",
    "        print(f\"Epoch {epoch} Loss: {loss}\")\n",
    "\n",
    "# Final prediction\n",
    "print(\"\\nFinal output after training:\")\n",
    "print(a2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (pytorch_env)",
   "language": "python",
   "name": "pytorch_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
